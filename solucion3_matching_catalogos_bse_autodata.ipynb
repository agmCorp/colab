{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agmCorp/colab/blob/main/solucion3_matching_catalogos_bse_autodata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-8asNb1v9DU"
      },
      "source": [
        "# Sistema de Matching Optimizado v3: BSE vs Autodata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7NYKbNQv9DW"
      },
      "source": [
        "## 1️⃣ Instalación de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8cGVOuBv9DW"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Instalar librerías necesarias\n",
        "!pip install -q sentence-transformers pandas numpy unidecode tqdm rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK3q5P_Uv9DX"
      },
      "source": [
        "## 2️⃣ Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqx3mKPtv9DX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from rank_bm25 import BM25Okapi\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"Librerías importadas correctamente\")\n",
        "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR96_ywjv9DY"
      },
      "source": [
        "## 3️⃣ Cargar los archivos CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyxupIjtv9DY"
      },
      "outputs": [],
      "source": [
        "# Cargar los datos (datatype str para evitar problemas de lectura)\n",
        "df_bse = pd.read_csv(\n",
        "    '/content/sample_data/RAW_CART_MATRICERO_BSE_202602131851.csv',\n",
        "    dtype=str,\n",
        "    keep_default_na=False\n",
        " )\n",
        "df_autodata = pd.read_csv(\n",
        "    '/content/sample_data/RAW_CART_PAD_AUTODATA_202602131849.csv',\n",
        "    dtype=str,\n",
        "    keep_default_na=False\n",
        " )\n",
        "\n",
        "print(f\"Registros BSE: {len(df_bse):,}\")\n",
        "print(f\"Registros Autodata: {len(df_autodata):,}\")\n",
        "print(\"\\nPrimeras filas de BSE:\")\n",
        "display(df_bse.head(10))\n",
        "print(\"\\nPrimeras filas de Autodata:\")\n",
        "display(df_autodata.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeKhuJ35v9DZ"
      },
      "source": [
        "## 4️⃣ Funciones de Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxrREbG0ulzd"
      },
      "outputs": [],
      "source": [
        "# Caracteres extraños\n",
        "def detectar_caracteres_especiales(df, columna):\n",
        "    todos_los_textos = \" \".join(df[columna].astype(str).unique())\n",
        "    # Buscamos cualquier cosa que no sea letra, número, espacio o puntuación básica\n",
        "    caracteres_raros = set(re.findall(r'[^a-zA-Z0-9\\s.,\\-\\/]', todos_los_textos))\n",
        "    return caracteres_raros\n",
        "\n",
        "print(\"Caracteres extraños en BSE:\", detectar_caracteres_especiales(df_bse, 'MODELO_ORIGINAL'))\n",
        "print(\"Caracteres extraños en Autodata:\", detectar_caracteres_especiales(df_autodata, 'MODELO_A_ORIGINAL'))\n",
        "\n",
        "def estandarizar_terminos_tecnicos(texto):\n",
        "    \"\"\"Normaliza sinónimos técnicos a tokens estándar únicos .\"\"\"\n",
        "    if not texto: return \"\"\n",
        "    texto_procesado = texto.lower()\n",
        "\n",
        "    # Expandir abreviaturas con slash frecuentes\n",
        "    texto_procesado = re.sub(r'\\bc\\s*/\\s*', ' con ', texto_procesado)\n",
        "    texto_procesado = re.sub(r'\\bp\\s*/\\s*', ' para ', texto_procesado)\n",
        "\n",
        "    mapeos = {\n",
        "        r\"\\baire\\s+acondicionado\\b|\\baa\\b|\\ba/a\\b|\\baire\\b|\\bcon aire\\b\": \" aire_acondicionado \",\n",
        "        r\"\\bclimatizador\\b|\\bclim\\b|\\bclimaut\\b\": \" climatizador \",\n",
        "        r\"\\bpas\\b|\\bpax\\b|\\bpasajeros\\b\": \" pasajeros \",\n",
        "        r\"\\bhb\\b|\\bhatchback\\b\": \" hatchback \",\n",
        "        r\"\\bsb\\b|\\bsportback\\b\": \" sportback \",\n",
        "        r\"\\be\\.full\\b|\\bex\\.full\\b|\\bextra full\\b\": \" extra_full \",\n",
        "        r\"\\bs\\.full\\b|\\bsuper full\\b\": \" super_full \",\n",
        "        r\"\\bcue\\b|\\bcuero\\b\": \" cuero \",\n",
        "        r\"\\btech\\b|\\btecho\\b|\\bt\\.cielo\\b\": \" techo_solar \",\n",
        "        r\"\\bmb\\b|\\bmultim\\b|\\bmultimedia\\b\": \" multimedia \",\n",
        "        r\"\\bay\\.est\\b|\\bay\\.estac\\b|\\ba\\.estac\\b|\\bp\\.assi\\b|\\bp\\.assist\\b|\\bpark assist\\b\": \" ayuda_estacionamiento \",\n",
        "        r\"\\bllan\\b|\\bllantas\\b\": \" llantas \",\n",
        "        r\"\\btd\\b|\\bt\\.diesel\\b|\\bturbo d\\b\": \" turbo_diesel \",\n",
        "        r\"\\btdi\\b|\\btd intercooler\\b|\\bt\\.diesel intercooler\\b\": \" turbo_diesel_intercooler \",\n",
        "        r\"\\btiptronic\\b|\\bs-tronic\\b|\\baut\\.\\b|\\baut\\b|\\bautomatico\\b\": \" automatico \",\n",
        "        r\"\\b3-5p\\b|\\b3-5 ptas\\b|\\b3-5 puertas\\b\": \" 3_5_puertas \",\n",
        "        r\"\\b4-5p\\b|\\b4-5 ptas\\b|\\b4-5 puertas\\b\": \" 4_5_puertas \",\n",
        "        r\"(?<!-)(?:\\b2p\\b|\\b2 ptas\\b|\\b2 puertas\\b)\": \" 2_puertas \",\n",
        "        r\"(?<!-)(?:\\b3p\\b|\\b3 ptas\\b|\\b3 puertas\\b)\": \" 3_puertas \",\n",
        "        r\"(?<!-)(?:\\b4p\\b|\\b4 ptas\\b|\\b4 puertas\\b)\": \" 4_puertas \",\n",
        "        r\"(?<!-)(?:\\b5p\\b|\\b5 ptas\\b|\\b5 puertas\\b)\": \" 5_puertas \",\n",
        "        r\"\\badaptado a rural\\b|\\badaptada a rural\\b|\\bad\\.?\\s*rural\\b\": \" furgon_reformado \",\n",
        "        r\"\\blgo\\b|\\blargo\\b\": \" largo \",\n",
        "    }\n",
        "    for patron, reemplazo in mapeos.items():\n",
        "        texto_procesado = re.sub(patron, reemplazo, texto_procesado)\n",
        "    return texto_procesado\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    \"\"\"Repara caracteres extraños, unifica sinónimos y limpia símbolos.\"\"\"\n",
        "    if pd.isna(texto):\n",
        "        return \"\"\n",
        "\n",
        "    texto = str(texto).lower()\n",
        "\n",
        "    # 1. Reparación de caracteres extraños comunes\n",
        "    # IMPORTANTE: las secuencias multi-carácter (ã¡, ã©, ã³, ãº, ã±) deben ir\n",
        "    # ANTES que \"ã\" sola, para evitar que \"ã\" consuma el primer carácter de la secuencia.\n",
        "    MAPA_REPARACION = {\n",
        "        \"á\": \"a\",\n",
        "        \"é\": \"e\",\n",
        "        \"í\": \"i\",\n",
        "        \"ó\": \"o\",\n",
        "        \"ú\": \"u\",\n",
        "        \"ñ\": \"n\",\n",
        "        \"ã¡\": \"a\",\n",
        "        \"ã©\": \"e\",\n",
        "        \"ã³\": \"o\",\n",
        "        \"ãº\": \"u\",\n",
        "        \"ã±\": \"n\",\n",
        "        \"ã\": \"i\",\n",
        "        \"¿\": \"o\",\n",
        "        \"±\": \"n\",\n",
        "        \"ý\": \"i\",\n",
        "        \"ß\": \"a\",\n",
        "        \"ð\": \"o\",\n",
        "        \"â\": \"a\",\n",
        "        \"²\": \"2\",\n",
        "        \"³\": \"3\",\n",
        "        \"¬\": \"\",\n",
        "        \"·\": \"\",\n",
        "        \"°\": \"\",\n",
        "        \"º\": \"\",\n",
        "        \"*\": \"\",\n",
        "        \"+\": \"\",\n",
        "        \"|\": \"\",\n",
        "        \")\": \"\",\n",
        "        '\"': \"\",\n",
        "        \"!\": \"\",\n",
        "        \"]\": \"\",\n",
        "        \"[\": \"\",\n",
        "        \"(\": \"\",\n",
        "        \"?\": \"\",\n",
        "        \"=\": \"\",\n",
        "    }\n",
        "\n",
        "    for error, correccion in MAPA_REPARACION.items():\n",
        "        texto = texto.replace(error, correccion)\n",
        "\n",
        "    # 2. Unidecode y Estandarización\n",
        "    texto = unidecode(texto)\n",
        "    texto = estandarizar_terminos_tecnicos(texto)\n",
        "\n",
        "    # 3. Marcadores temporales para preservar contexto\n",
        "    decimal_token = \"_decimal_tok_\"\n",
        "    hyphen_token = \"_hyphen_tok_\"\n",
        "\n",
        "    # Preservar separadores decimales (ej: 1.5, 2,0)\n",
        "    texto = re.sub(r\"(?<=\\d)[\\.,](?=\\d)\", decimal_token, texto)\n",
        "    # Preservar guiones válidos entre alfanuméricos (códigos y rangos: x-line, s-tronic)\n",
        "    texto = re.sub(r'(?<=[a-z0-9])-(?=[a-z0-9])', hyphen_token, texto)\n",
        "\n",
        "\n",
        "    # Expandir slash técnico entre alfanuméricos a '_' (dx/lx -> dx_lx)\n",
        "    texto = re.sub(r'(?<=[a-z0-9])/(?=[a-z0-9])', '_', texto)\n",
        "\n",
        "    # 4. Separar solo guión editorial (cuando aparece con espacios: 'full - extrafull')\n",
        "    texto = re.sub(r'\\s+-\\s+', ' ', texto)\n",
        "\n",
        "    # 5. Limpieza final: solo letras, números y guion bajo\n",
        "    texto = re.sub(r\"[^a-z0-9_\\s]\", \" \", texto)\n",
        "\n",
        "    # 6. Restaurar marcadores temporales\n",
        "    texto = texto.replace(hyphen_token, '-')\n",
        "    texto = texto.replace(decimal_token, '.')\n",
        "    return re.sub(r\"\\s+\", \" \", texto).strip()\n",
        "\n",
        "def corregir_anio_bse(anio):\n",
        "    \"\"\"Corrige años > 3000 restándoles 1000 (ej: 3020 -> 2020)\"\"\"\n",
        "    if anio is None: return None\n",
        "    return anio - 1000 if anio >= 3000 else anio\n",
        "\n",
        "def extraer_anio_inicio_fin(rango_anios):\n",
        "    if pd.isna(rango_anios): return None, None\n",
        "    rango_str = str(rango_anios).strip()\n",
        "\n",
        "    # Intenta encontrar dos años (rango)\n",
        "    match = re.findall(r'(\\d{4})', rango_str)\n",
        "    if len(match) >= 2:\n",
        "        return corregir_anio_bse(int(match[0])), corregir_anio_bse(int(match[1]))\n",
        "    elif len(match) == 1:\n",
        "        anio = corregir_anio_bse(int(match[0]))\n",
        "        return anio, anio\n",
        "    return None, None\n",
        "\n",
        "def normalizar_marca(texto):\n",
        "    if pd.isna(texto): return \"\"\n",
        "    texto = str(texto).lower()\n",
        "    # Elimina cualquier contenido entre picos <...>\n",
        "    texto = re.sub(r'<.*?>', '', texto)\n",
        "    return texto.strip()\n",
        "\n",
        "# ========== TABLA DE ALIAS DE MARCAS ==========\n",
        "# Mapea marca normalizada de Autodata → lista de marcas normalizadas en BSE.\n",
        "# Soporta mapeo uno-a-muchos para marcas compuestas (ej: DONGFENG → busca en 2 marcas BSE).\n",
        "# Si la marca Autodata no está en este diccionario, se usa tal cual (match directo).\n",
        "ALIAS_MARCAS = {\n",
        "    # --- Espaciado diferente ---\n",
        "    \"polarsun\":         [\"polar sun\"],\n",
        "    \"routebuggy\":       [\"route buggy\"],\n",
        "    \"zhong tong\":       [\"zhongtong\"],\n",
        "    \"zxauto\":           [\"zx auto\"],\n",
        "    \"tongbao\":          [\"tong bao\"],\n",
        "    \"leapmotor\":        [\"leap motor\"],\n",
        "    # --- Nombre extendido / abreviado ---\n",
        "    \"kia motors\":       [\"kia\"],\n",
        "    \"byd auto\":         [\"byd\"],\n",
        "    \"bjc\":              [\"bjc-beijing\"],\n",
        "    \"gwm\":              [\"gwm - [great wall motor]\"],\n",
        "    \"gac - trumpchi\":   [\"gac\"],\n",
        "    \"lynk co\":          [\"lynk y co\"],\n",
        "    \"porsche replica\":  [\"porsche\"],\n",
        "    \"brilliance sunra\": [\"brilliance\"],\n",
        "    # --- Typos en BSE (no se pueden corregir en CSV fuente) ---\n",
        "    \"shineray\":         [\"shinerai\"],\n",
        "    \"fuqi\":             [\"fuqui\"],\n",
        "    # --- Marca compuesta → una marca BSE ---\n",
        "    \"dfsk\":             [\"dfm - dfsk (dong feng)\"],\n",
        "    \"chana\":            [\"chana-changan\"],\n",
        "    \"changan\":          [\"chana-changan\"],\n",
        "    \"geely riddara\":    [\"riddara (geely)\"],\n",
        "    # --- Marca compuesta → múltiples marcas BSE ---\n",
        "    \"dongfeng\":         [\"dfm - dfsk (dong feng)\", \"forthing (dongfeng)\"],\n",
        "    \"daewoo - fso\":     [\"daewoo\", \"fso\"],\n",
        "    \"vaz 1111 / uaz\":   [\"vaz\", \"uaz\"],\n",
        "}\n",
        "\n",
        "def resolver_marcas_bse(marca_auto_norm):\n",
        "    \"\"\"Resuelve una marca normalizada de Autodata a lista de marcas BSE equivalentes.\"\"\"\n",
        "    if marca_auto_norm in ALIAS_MARCAS:\n",
        "        return ALIAS_MARCAS[marca_auto_norm]\n",
        "    return [marca_auto_norm]\n",
        "\n",
        "def validar_normalizacion_contextual(df_bse, df_autodata, n_muestra=50):\n",
        "    \"\"\"Genera validación rápida con tabla de ejemplos y muestra real para revisión manual.\"\"\"\n",
        "    ejemplos = [\n",
        "        (\"c/diesel 2,0\", \"con diesel 2.0\"),\n",
        "        (\"p/lancha x-line\", \"para lancha x-line\"),\n",
        "        (\"S-TRONIC 3.3-3.8\", \"automatico 3.3-3.8\"),\n",
        "        (\"USM-2450-74-ST\", \"usm-2450-74-st\"),\n",
        "        (\"FULL - EXTRAFULL\", \"full extrafull\"),\n",
        "        (\"DX/LX 4-5p\", \"dx_lx 4_5_puertas\"),\n",
        "        (\"Adaptado a rural\", \"furgon_reformado\"),\n",
        "        (\"ad. rural\", \"furgon_reformado\"),\n",
        "    ]\n",
        "\n",
        "    df_ejemplos = pd.DataFrame(ejemplos, columns=['entrada', 'salida_esperada'])\n",
        "    df_ejemplos['salida_obtenida'] = df_ejemplos['entrada'].apply(normalizar_texto)\n",
        "    df_ejemplos['ok'] = df_ejemplos['salida_obtenida'] == df_ejemplos['salida_esperada']\n",
        "\n",
        "    serie_real = pd.concat([\n",
        "        df_bse['MODELO_ORIGINAL'].astype(str),\n",
        "        df_autodata['MODELO_A_ORIGINAL'].astype(str)\n",
        "    ], ignore_index=True).dropna().drop_duplicates()\n",
        "\n",
        "    n = min(n_muestra, len(serie_real))\n",
        "    df_revision = pd.DataFrame({'entrada_real': serie_real.sample(n=n, random_state=42)})\n",
        "    df_revision['salida_normalizada'] = df_revision['entrada_real'].apply(normalizar_texto)\n",
        "\n",
        "    return df_ejemplos, df_revision\n",
        "\n",
        "print(\"Funciones de preprocesamiento definidas\")\n",
        "print(f\"Alias de marcas configurados: {len(ALIAS_MARCAS)} reglas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21uFfU29v9Da"
      },
      "source": [
        "## 5️⃣ Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZMFXDuav9Da"
      },
      "outputs": [],
      "source": [
        "print(\"Iniciando preprocesamiento de catálogos...\")\n",
        "\n",
        "# --- PROCESAR BSE ---\n",
        "df_bse['marca_norm'] = df_bse['MARCA'].apply(normalizar_marca)\n",
        "df_bse['modelo_norm'] = df_bse['MODELO_ORIGINAL'].apply(normalizar_texto)\n",
        "df_bse[['anio_inicio', 'anio_fin']] = df_bse['RANGO_ANIOS'].apply(\n",
        "    lambda x: pd.Series(extraer_anio_inicio_fin(x))\n",
        ")\n",
        "# Filtrar registros sin años procesables\n",
        "df_bse = df_bse[df_bse['anio_inicio'].notna()].copy()\n",
        "df_bse.reset_index(drop=True, inplace=True)  # IMPORTANTE: resetear índice para indexación consistente con embeddings\n",
        "\n",
        "# --- PROCESAR AUTODATA ---\n",
        "df_autodata['marca_norm'] = df_autodata['MARCA_A'].apply(normalizar_marca)\n",
        "df_autodata['modelo_norm'] = df_autodata['MODELO_A_ORIGINAL'].apply(normalizar_texto)\n",
        "# Año técnico para matching (derivado desde texto)\n",
        "df_autodata['anio'] = pd.to_numeric(df_autodata['ANIO_A'], errors='coerce').fillna(9999).astype(int)\n",
        "\n",
        "print(\"Preprocesamiento completado.\")\n",
        "print(f\"BSE limpio: {len(df_bse):,} registros | Autodata: {len(df_autodata):,} registros.\")\n",
        "\n",
        "print(\"\\nEjemplo de preprocesamiento:\")\n",
        "print(\"\\nBSE:\")\n",
        "display(df_bse[['MARCA', 'marca_norm', 'MODELO_ORIGINAL', 'modelo_norm', 'RANGO_ANIOS', 'anio_inicio', 'anio_fin']].head(10))\n",
        "print(\"\\nAutodata:\")\n",
        "display(df_autodata[['MARCA_A', 'marca_norm', 'MODELO_A_ORIGINAL', 'modelo_norm', 'ANIO_A', 'anio']].head(10))\n",
        "\n",
        "# Validación recomendada antes de ejecutar matching completo\n",
        "df_validacion_ejemplos, df_revision_50 = validar_normalizacion_contextual(df_bse, df_autodata, n_muestra=50)\n",
        "print(\"\\nValidación de ejemplos controlados:\")\n",
        "display(df_validacion_ejemplos)\n",
        "print(\"\\nMuestra de 50 casos reales para revisión manual:\")\n",
        "display(df_revision_50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbIu43lcv9Db"
      },
      "source": [
        "## 6️⃣ Cargar Modelos y Construir Índices Optimizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6yz9P4CslvL"
      },
      "outputs": [],
      "source": [
        "# Detectar dispositivo para los modelos de IA (Transformers sí usarán GPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Usando device para IA: {device}\")\n",
        "\n",
        "# ========== 1. MODELOS DE IA ==========\n",
        "print(\"\\n[1/4] Cargando modelos de embeddings y reranker...\")\n",
        "# BGE-M3 genera los vectores y Reranker decide el mejor candidato\n",
        "model_bi = SentenceTransformer('BAAI/bge-m3', device=device)\n",
        "reranker = CrossEncoder('BAAI/bge-reranker-v2-m3', device=device)\n",
        "print(\"Modelos cargados\")\n",
        "\n",
        "# ========== 2. ÍNDICE POR MARCA + AÑO (Pre-filtrado inteligente) ==========\n",
        "print(\"\\n[2/4] Construyendo índice de marca-año para filtrado rápido...\")\n",
        "# Diccionario: marca -> {año -> [índices]}\n",
        "marca_anio_index = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for idx, row in df_bse.iterrows():\n",
        "    marca = row['marca_norm']\n",
        "    # Aseguramos que sean enteros\n",
        "    try:\n",
        "        anio_ini = int(row['anio_inicio'])\n",
        "        anio_fin = int(row['anio_fin'])\n",
        "        # Agregamos este registro a todos los años en su rango\n",
        "        for year in range(anio_ini, anio_fin + 1):\n",
        "            marca_anio_index[marca][year].append(idx)\n",
        "    except (ValueError, TypeError):\n",
        "        continue # Si hay datos sucios en años, los saltamos\n",
        "\n",
        "print(f\"Índice construido: {len(marca_anio_index)} marcas\")\n",
        "\n",
        "# ========== 3. EMBEDDINGS (Búsqueda vectorial) ==========\n",
        "print(\"\\n[3/4] Generando embeddings BGE-M3 para BSE...\")\n",
        "bse_textos = (df_bse['marca_norm'] + \" \" + df_bse['modelo_norm']).tolist()\n",
        "\n",
        "# Generar embeddings (Esto usa GPU si está disponible, gracias a model_bi)\n",
        "bse_embeddings = model_bi.encode(\n",
        "    bse_textos,\n",
        "    convert_to_tensor=False,  # numpy para dot-product directo\n",
        "    batch_size=512,\n",
        "    show_progress_bar=True,\n",
        "    normalize_embeddings=True  # Normalizar para que Inner Product = Cosine Similarity\n",
        ")\n",
        "\n",
        "# Convertir a float32 para operaciones numéricas eficientes\n",
        "bse_embeddings = bse_embeddings.astype('float32')\n",
        "\n",
        "print(f\"Embeddings generados: {bse_embeddings.shape[0]} vectores ({bse_embeddings.shape[1]}D)\")\n",
        "\n",
        "# ========== 4. ÍNDICES BM25 POR MARCA ==========\n",
        "print(\"\\n[4/4] Construyendo índices BM25 por marca...\")\n",
        "bm25_cache = {}\n",
        "marca_to_indices = defaultdict(list)\n",
        "\n",
        "# Agrupar índices por marca\n",
        "for idx, row in df_bse.iterrows():\n",
        "    marca_to_indices[row['marca_norm']].append(idx)\n",
        "\n",
        "# Crear un BM25 por marca\n",
        "for marca, indices in tqdm(marca_to_indices.items(), desc=\"BM25 por marca\"):\n",
        "    # Tokenizamos simplemente por espacios para BM25\n",
        "    corpus_marca = [str(df_bse.loc[i, 'modelo_norm']).split() for i in indices]\n",
        "    bm25_cache[marca] = {\n",
        "        'bm25': BM25Okapi(corpus_marca),\n",
        "        'indices': indices # Guardamos los índices reales de dataframe que corresponden al BM25 local\n",
        "    }\n",
        "\n",
        "print(f\"{len(bm25_cache)} índices BM25 cacheados\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SISTEMA OPTIMIZADO LISTO\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL2PuZANv9Db"
      },
      "source": [
        "## 7️⃣ Función de Matching Optimizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUj8ptC3s0Wi"
      },
      "outputs": [],
      "source": [
        "def calcular_penalizacion_anio(anio_query, anio_ini_bse, anio_fin_bse):\n",
        "    \"\"\"\n",
        "    Calcula penalización gradual basada en qué tan lejos está el año.\n",
        "    Retorna multiplicador [0.7 a 1.0]\n",
        "    \"\"\"\n",
        "    if anio_query == 9999:  # Año desconocido\n",
        "        return 0.9  # Penalización leve por incertidumbre\n",
        "\n",
        "    # Dentro del rango = sin penalización\n",
        "    if anio_ini_bse <= anio_query <= anio_fin_bse:\n",
        "        return 1.0\n",
        "\n",
        "    # Calcular distancia mínima al rango\n",
        "    if anio_query < anio_ini_bse:\n",
        "        distancia = anio_ini_bse - anio_query\n",
        "    else:\n",
        "        distancia = anio_query - anio_fin_bse\n",
        "\n",
        "    # Penalización: 1 año = 0.95, 2 años = 0.85, 3+ años = 0.7\n",
        "    if distancia == 1:\n",
        "        return 0.95\n",
        "    elif distancia == 2:\n",
        "        return 0.85\n",
        "    else:\n",
        "        return 0.7\n",
        "\n",
        "def sigmoid_temperatura(x, temperatura=2.5):\n",
        "    \"\"\"Convierte logits a [0,1] con escala fija para comparabilidad entre registros.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-x / temperatura))\n",
        "\n",
        "def encontrar_matches_optimizado(row_autodata, top_k=3, top_candidates=50):\n",
        "    \"\"\"\n",
        "    Pipeline optimizado en 3 fases:\n",
        "    1. Pre-filtrado inteligente (marca + año)\n",
        "    2. Búsqueda híbrida (semántica directa + BM25 léxico)\n",
        "    3. Reranking con cross-encoder\n",
        "    \"\"\"\n",
        "    marca_auto = row_autodata['marca_norm']\n",
        "    anio_auto = row_autodata['anio']\n",
        "    modelo_auto = row_autodata['modelo_norm']\n",
        "    query_text = f\"{marca_auto} {modelo_auto}\"\n",
        "\n",
        "    # ========== FASE 1: PRE-FILTRADO INTELIGENTE ==========\n",
        "    # 1.1 Resolver alias de marca (puede mapear a múltiples marcas BSE)\n",
        "    marcas_bse = resolver_marcas_bse(marca_auto)\n",
        "\n",
        "    # 1.1b Obtener candidatos por marca(s)\n",
        "    indices_marca = []\n",
        "    for m in marcas_bse:\n",
        "        if m in marca_to_indices:\n",
        "            indices_marca.extend(marca_to_indices[m])\n",
        "\n",
        "    if not indices_marca:\n",
        "        return []  # Ninguna marca resuelta existe en BSE\n",
        "\n",
        "    # 1.2 Filtrar por año con ventana de ±2 años\n",
        "    if anio_auto != 9999:\n",
        "        indices_candidatos = set()\n",
        "        for m in marcas_bse:\n",
        "            if m in marca_anio_index:\n",
        "                for offset in range(-2, 3):\n",
        "                    year = anio_auto + offset\n",
        "                    if year in marca_anio_index[m]:\n",
        "                        indices_candidatos.update(marca_anio_index[m][year])\n",
        "\n",
        "        indices_candidatos = list(indices_candidatos)\n",
        "\n",
        "        # Si el filtro fue muy agresivo, usar todos de la marca\n",
        "        if len(indices_candidatos) < 10:\n",
        "            indices_candidatos = indices_marca\n",
        "    else:\n",
        "        indices_candidatos = indices_marca\n",
        "\n",
        "    if not indices_candidatos:\n",
        "        return []\n",
        "\n",
        "    # ========== FASE 2: BÚSQUEDA HÍBRIDA ==========\n",
        "    # 2.1 Búsqueda semántica directa sobre candidatos pre-filtrados\n",
        "    query_embedding = model_bi.encode(\n",
        "        [query_text],\n",
        "        normalize_embeddings=True\n",
        "    ).astype('float32')\n",
        "\n",
        "    # Calcular similitud solo contra los candidatos pre-filtrados (evita truncamiento por k global)\n",
        "    candidate_indices = np.array(indices_candidatos)\n",
        "    candidate_embeddings = bse_embeddings[candidate_indices]\n",
        "    similarities = np.dot(candidate_embeddings, query_embedding.T).flatten()\n",
        "\n",
        "    # Tomar los mejores candidatos semánticos\n",
        "    top_sem_count = min(top_candidates * 2, len(candidate_indices))\n",
        "    top_sem_positions = np.argsort(similarities)[::-1][:top_sem_count]\n",
        "    semantic_results = [(int(candidate_indices[pos]), float(similarities[pos])) for pos in top_sem_positions]\n",
        "\n",
        "    if not semantic_results:\n",
        "        return []\n",
        "\n",
        "    # 2.2 Búsqueda léxica con BM25 (consulta en todas las marcas resueltas)\n",
        "    bm25_dict = {}\n",
        "    tokenized_query = modelo_auto.split()\n",
        "    for m in marcas_bse:\n",
        "        if m in bm25_cache:\n",
        "            bm25_data = bm25_cache[m]\n",
        "            bm25_model = bm25_data['bm25']\n",
        "            bm25_indices_locales = bm25_data['indices']\n",
        "\n",
        "            bm25_scores = bm25_model.get_scores(tokenized_query)\n",
        "\n",
        "            # Normalizar BM25 dentro de cada marca independientemente\n",
        "            if bm25_scores.max() > 0:\n",
        "                bm25_scores = bm25_scores / bm25_scores.max()\n",
        "\n",
        "            # Merge: conservar el mayor score por índice (para multi-marca)\n",
        "            for idx, score in zip(bm25_indices_locales, bm25_scores):\n",
        "                if idx not in bm25_dict or score > bm25_dict[idx]:\n",
        "                    bm25_dict[idx] = score\n",
        "\n",
        "    # 2.3 Combinar scores semántico + léxico + penalización de año\n",
        "    combined_scores = []\n",
        "    for idx, sem_score in semantic_results:\n",
        "        lex_score = bm25_dict.get(idx, 0)\n",
        "\n",
        "        # Penalización por diferencia de año\n",
        "        anio_ini = df_bse.loc[idx, 'anio_inicio']\n",
        "        anio_fin = df_bse.loc[idx, 'anio_fin']\n",
        "        year_penalty = calcular_penalizacion_anio(anio_auto, anio_ini, anio_fin)\n",
        "\n",
        "        # Score híbrido: (70% semántico + 30% léxico) × penalización_año (multiplicativa)\n",
        "        hybrid_score = (0.70 * sem_score + 0.30 * lex_score) * year_penalty\n",
        "        combined_scores.append((idx, hybrid_score))\n",
        "\n",
        "    # Ordenar por score híbrido\n",
        "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_hybrid = combined_scores[:top_candidates]\n",
        "\n",
        "    if not top_hybrid:\n",
        "        return []\n",
        "\n",
        "    # ========== FASE 3: RERANKING CON CROSS-ENCODER ==========\n",
        "    indices_finales = [idx for idx, _ in top_hybrid]\n",
        "    pairs = [\n",
        "        [\n",
        "            f\"{query_text} {anio_auto if anio_auto != 9999 else ''}\",\n",
        "            f\"{df_bse.loc[idx, 'marca_norm']} {df_bse.loc[idx, 'modelo_norm']} {int(df_bse.loc[idx, 'anio_inicio'])}-{int(df_bse.loc[idx, 'anio_fin'])}\"\n",
        "        ]\n",
        "        for idx in indices_finales\n",
        "    ]\n",
        "\n",
        "    rerank_scores = reranker.predict(pairs)\n",
        "\n",
        "    # SCORE LOCAL (ranking dentro del mismo registro)\n",
        "    min_score = rerank_scores.min()\n",
        "    max_score = rerank_scores.max()\n",
        "\n",
        "    if max_score > min_score:\n",
        "        normalized_scores = (rerank_scores - min_score) / (max_score - min_score)\n",
        "    else:\n",
        "        normalized_scores = np.ones_like(rerank_scores) * 0.5\n",
        "\n",
        "    hybrid_scores_raw = np.array([score for _, score in top_hybrid], dtype=float)\n",
        "    if hybrid_scores_raw.max() > 0:\n",
        "        hybrid_scores_local = hybrid_scores_raw / hybrid_scores_raw.max()\n",
        "    else:\n",
        "        hybrid_scores_local = np.zeros_like(hybrid_scores_raw)\n",
        "\n",
        "    final_scores = 0.8 * normalized_scores + 0.2 * hybrid_scores_local\n",
        "\n",
        "    # SCORE_COMPARABLE (comparable entre registros)\n",
        "    rerank_global = sigmoid_temperatura(rerank_scores, temperatura=2.5)\n",
        "    hybrid_global = np.clip(hybrid_scores_raw, 0, 1)\n",
        "    score_comparable = 0.85 * rerank_global + 0.15 * hybrid_global\n",
        "\n",
        "    # Top K final\n",
        "    top_k_indices = np.argsort(final_scores)[::-1][:top_k]\n",
        "\n",
        "    return [{\n",
        "        'ID_AUTODATA_BSE': df_bse.loc[indices_finales[i], 'ID_AUTODATA_BSE'],\n",
        "        'score_comparable': round(float(score_comparable[i]), 4),\n",
        "        'score': round(float(final_scores[i]), 4),\n",
        "        'marca_bse': df_bse.loc[indices_finales[i], 'MARCA'],\n",
        "        'modelo_bse': df_bse.loc[indices_finales[i], 'MODELO_ORIGINAL'],\n",
        "        'rango_anios_bse': df_bse.loc[indices_finales[i], 'RANGO_ANIOS']\n",
        "    } for i in top_k_indices]\n",
        "\n",
        "print(\"Función de matching optimizada definida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_8ngLyev9Dc"
      },
      "source": [
        "## 8️⃣ Ejecutar matching para todo el catálogo Autodata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMcqJ78Cv9Dd"
      },
      "outputs": [],
      "source": [
        "TOP_K = 3\n",
        "resultados_finales = []\n",
        "\n",
        "# Lookup O(1) para COMB y TIPO por ID_AUTODATA_BSE\n",
        "bse_lookup = df_bse.drop_duplicates(subset='ID_AUTODATA_BSE').set_index('ID_AUTODATA_BSE')[['COMB', 'TIPO']].to_dict('index')\n",
        "\n",
        "print(f\"Iniciando matching de {len(df_autodata):,} registros...\")\n",
        "print(\"Estimado: ~120-150 minutos en T4 GPU\\n\")\n",
        "\n",
        "# Procesamiento con barra de progreso\n",
        "for idx, row in tqdm(df_autodata.iterrows(), total=len(df_autodata), desc=\"Matching\"):\n",
        "    matches = encontrar_matches_optimizado(row, top_k=TOP_K)\n",
        "\n",
        "    # Estructura base homogénea para todas las filas\n",
        "    res = {\n",
        "        'ID_AUTADATA': row['ID_AUTADATA'],\n",
        "        'MARCA_A': row['MARCA_A'],\n",
        "        'MODELO_A_ORIGINAL': row['MODELO_A_ORIGINAL'],\n",
        "        'ANIO_A': row['ANIO_A'],\n",
        "\n",
        "        'SCORE_COMPARABLE': 0.0,\n",
        "        'SCORE_1': 0.0,\n",
        "        'SCORE_2': 0.0,\n",
        "        'SCORE_3': 0.0,\n",
        "\n",
        "        'ID_AUTODATA_BSE_1': 'NO_MATCH',\n",
        "        'ID_AUTODATA_BSE_2': '',\n",
        "        'ID_AUTODATA_BSE_3': '',\n",
        "\n",
        "        'MARCA_BSE_1': '',\n",
        "        'MARCA_BSE_2': '',\n",
        "        'MARCA_BSE_3': '',\n",
        "\n",
        "        'MODELO_BSE_1': '',\n",
        "        'MODELO_BSE_2': '',\n",
        "        'MODELO_BSE_3': '',\n",
        "\n",
        "        'RANGO_ANIOS_BSE_1': '',\n",
        "        'RANGO_ANIOS_BSE_2': '',\n",
        "        'RANGO_ANIOS_BSE_3': '',\n",
        "\n",
        "        'COMB_BSE': '',\n",
        "        'TIPO_BSE': ''\n",
        "    }\n",
        "\n",
        "    if matches:\n",
        "        res['SCORE_COMPARABLE'] = matches[0].get('score_comparable', 0.0)\n",
        "\n",
        "        # Llenar con los Top K matches encontrados\n",
        "        for i, m in enumerate(matches[:TOP_K]):\n",
        "            suffix = f\"_{i+1}\"\n",
        "            res[f'SCORE{suffix}'] = m['score']\n",
        "            res[f'ID_AUTODATA_BSE{suffix}'] = m['ID_AUTODATA_BSE']\n",
        "            res[f'MARCA_BSE{suffix}'] = m['marca_bse']\n",
        "            res[f'MODELO_BSE{suffix}'] = m['modelo_bse']\n",
        "            res[f'RANGO_ANIOS_BSE{suffix}'] = m['rango_anios_bse']\n",
        "\n",
        "        # Campos de trazabilidad solo para el primer match\n",
        "        first_match_bse_id = matches[0]['ID_AUTODATA_BSE']\n",
        "\n",
        "        # Verificar si el ID es válido (string no vacío) y buscar en lookup O(1)\n",
        "        if first_match_bse_id and not pd.isna(first_match_bse_id) and str(first_match_bse_id).strip() != '':\n",
        "            bse_id_str = str(first_match_bse_id)\n",
        "            if bse_id_str in bse_lookup:\n",
        "                res['COMB_BSE'] = bse_lookup[bse_id_str].get('COMB', '')\n",
        "                res['TIPO_BSE'] = bse_lookup[bse_id_str].get('TIPO', '')\n",
        "\n",
        "    resultados_finales.append(res)\n",
        "\n",
        "# Crear DataFrame\n",
        "df_resultados = pd.DataFrame(resultados_finales)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MATCHING COMPLETADO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total procesado: {len(df_resultados):,} registros\")\n",
        "print(f\"Sin matches: {(df_resultados['SCORE_1'] == 0).sum():,}\")\n",
        "print(f\"Con matches: {(df_resultados['SCORE_1'] > 0).sum():,}\")\n",
        "print(f\"\\nScore promedio: {df_resultados[df_resultados['SCORE_1'] > 0]['SCORE_1'].mean():.3f}\")\n",
        "\n",
        "display(df_resultados.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4lK1ygKv9De"
      },
      "source": [
        "## 9️⃣ Análisis de Calidad y Exportación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nYvRTmiv9De"
      },
      "outputs": [],
      "source": [
        "print(\"\\nDISTRIBUCIÓN DE SCORE_COMPARABLE (GLOBAL):\\n\")\n",
        "df_con_match_global = df_resultados[df_resultados['SCORE_COMPARABLE'] > 0]\n",
        "\n",
        "print(f\"Score >= 0.9 (Alta confianza):  {(df_con_match_global['SCORE_COMPARABLE'] >= 0.9).sum():,}\")\n",
        "print(f\"Score 0.7-0.9 (Media confianza): {((df_con_match_global['SCORE_COMPARABLE'] >= 0.7) & (df_con_match_global['SCORE_COMPARABLE'] < 0.9)).sum():,}\")\n",
        "print(f\"Score 0.5-0.7 (Revision humana):         {((df_con_match_global['SCORE_COMPARABLE'] >= 0.5) & (df_con_match_global['SCORE_COMPARABLE'] < 0.7)).sum():,}\")\n",
        "print(f\"Score < 0.5 (Baja confianza):    {(df_con_match_global['SCORE_COMPARABLE'] < 0.5).sum():,}\")\n",
        "\n",
        "# Exportar resultados\n",
        "archivo_salida = 'resultados_matching_completo_v3.csv'\n",
        "df_resultados.to_csv(archivo_salida, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"\\nResultados exportados a: {archivo_salida}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"========= PROCESO COMPLETADO EXITOSAMENTE =========\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "977481c3"
      },
      "outputs": [],
      "source": [
        "columnas_origen = [\n",
        "    'ID_AUTADATA',\n",
        "    'ID_AUTODATA_BSE_1',\n",
        "    'SCORE_COMPARABLE',\n",
        "    'SCORE_1',\n",
        "    'MARCA_A',\n",
        "    'MARCA_BSE_1',\n",
        "    'ANIO_A',\n",
        "    'RANGO_ANIOS_BSE_1',\n",
        "    'MODELO_A_ORIGINAL',\n",
        "    'MODELO_BSE_1',\n",
        "    'COMB_BSE',\n",
        "    'TIPO_BSE'\n",
        "]\n",
        "\n",
        "renombrar_columnas = {\n",
        "    'ID_AUTADATA': 'ID_AUTODATA',\n",
        "    'ID_AUTODATA_BSE_1': 'ID_AUTODATA_BSE',\n",
        "    'SCORE_1': 'SCORE_LOCAL',\n",
        "    'MARCA_BSE_1': 'MARCA_BSE',\n",
        "    'RANGO_ANIOS_BSE_1': 'RANGO_ANIOS_BSE',\n",
        "    'MODELO_A_ORIGINAL': 'MODELO_A',\n",
        "    'MODELO_BSE_1': 'MODELO_BSE'\n",
        "}\n",
        "\n",
        "df_filtrado = df_resultados[columnas_origen].rename(columns=renombrar_columnas).copy()\n",
        "print(\"DataFrame 'df_filtrado' creado con columnas finales estandarizadas.\")\n",
        "display(df_filtrado.head(10))\n",
        "\n",
        "archivo_filtrado_salida = \"resultados_matching_resumido_v3.csv\"\n",
        "df_filtrado.to_csv(archivo_filtrado_salida, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"DataFrame filtrado exportado a: {archivo_filtrado_salida}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}